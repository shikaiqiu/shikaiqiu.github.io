<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Shikai Qiu 裘释凯</title> <meta name="author" content="Shikai Qiu 裘释凯"/> <meta name="description" content="shikai qiu, machine learning, PhD, NYU, Berkeley"/> <meta name="keywords" content="shikai qiu, machine learning, PhD, NYU, Berkeley"/> <meta name="google-site-verification" content="WnmZFbv7JOouTwBMfFEckg56TRCt4LSai-5AmmAggWw"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Lato:wght@300;400;700&family=Source+Sans+Pro:wght@300;400;600;700&family=Crimson+Pro:wght@300;400;500;600&family=Spectral:wght@300;400;500;600&display=swap"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://shikaiqiu.github.io/"> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <meta name="google-site-verification" content="CORreV28s411oRM53g1NPyCO22M68ZuBC6qMvsDlCYk"/> </head> <body class="fixed-top-nav "> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="color: var(--global-theme-color); font-weight: normal;"> Shikai Qiu 裘释凯 </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/photo-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/photo-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/photo-1400.webp"></source> <img src="/assets/img/photo.png" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="photo.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>I’m a fourth-year PhD student in Computer Science at <a href="https://cs.nyu.edu/home/index.html" target="_blank" rel="noopener noreferrer">NYU Courant</a>, working with <a href="https://cims.nyu.edu/~andrewgw/" target="_blank" rel="noopener noreferrer">Andrew Gordon Wilson</a>. I’m interested in the science of scaling neural networks. You can reach me at <a href="">sq2129@nyu.edu</a>.</p> <p>I’m supported by the Two Sigma PhD Fellowship. Previously, I was a Student Researcher at Google Research with <a href="https://www.nikunjsaunshi.com/" target="_blank" rel="noopener noreferrer">Nikunj Saunshi</a> and <a href="https://www.cs.cmu.edu/~elan/" target="_blank" rel="noopener noreferrer">Elan Rosenfeld</a> (2025), working on looped transformers for LLM reasoning, and at Google DeepMind with <a href="https://scholar.google.com/citations?user=cn_FoswAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Jeffrey Pennington</a> and <a href="https://ati.sh/bio/" target="_blank" rel="noopener noreferrer">Atish Agarwala</a> (2024), where our work on <a href="https://arxiv.org/abs/2507.02119" target="_blank" rel="noopener noreferrer">scaling collapse</a> received an ICML Oral. I also interned at <a href="https://aws.amazon.com/" target="_blank" rel="noopener noreferrer">Amazon AWS</a> (2023) and <a href="https://ai.meta.com/" target="_blank" rel="noopener noreferrer">Meta AI</a> (2022).</p> <p>I studied Physics and Computer Science at UC Berkeley, where I worked with <a href="http://www.jennifer.listgarten.com/" target="_blank" rel="noopener noreferrer">Jennifer Listgarten</a> on equivariant neural networks for drug discovery, and <a href="https://hwang43.web.cern.ch/" target="_blank" rel="noopener noreferrer">Haichen Wang</a> and <a href="https://nachmangroup.github.io/" target="_blank" rel="noopener noreferrer">Ben Nachman</a> on deep learning for high energy physics and search for physics beyond the Standard Model. I co-received the <a href="https://breakthroughprize.org/Laureates/1/L3993" target="_blank" rel="noopener noreferrer">2025 Breakthrough Prize in Fundamental Physics</a> via the ATLAS Collaboration at CERN.</p> </div> <div class="publications"> <h2 style="color: var(--global-theme-color); font-weight: normal;">Selected Publications</h2> <p>* Equal contribution</p> <ol class="bibliography"> <li> <div class="row"> <div id="finzi2026epiplexity" class="col-sm-12"> <div class="title" style="color: var(--global-theme-color);">From Entropy to Epiplexity: Rethinking Information for Computationally Bounded Intelligence</div> <div class="author"> Marc Finzi*, <span style="color: var(--global-theme-color); text-decoration: none; font-weight: 500;">Shikai Qiu*</span>, Yiding Jiang*, Pavel Izmailov, J. Zico Kolter, and Andrew Gordon Wilson</div> <div class="periodical"> <em>arXiv preprint arXiv:2601.03220</em> 2026 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2601.03220" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a> </div> <div class="abstract hidden"> <p>Can we learn more from data than existed in the generating process itself? Can new and useful information be constructed from merely applying deterministic transformations to existing data? On these questions, Shannon information and Kolmogorov complexity come up nearly empty-handed, in part because they assume observers with unlimited computational capacity and fail to target the useful information content. We introduce epiplexity, a formalization of information capturing what computationally bounded observers can learn from data. Epiplexity captures the structural content in data while excluding time-bounded entropy, the random unpredictable content exemplified by pseudorandom number generators and chaotic dynamical systems. We present practical procedures to estimate epiplexity which capture differences across data sources, track with downstream performance, and highlight dataset interventions that improve out-of-distribution generalization.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="qiu2025hyperparameter" class="col-sm-12"> <div class="title" style="color: var(--global-theme-color);">Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales</div> <div class="author"> <span style="color: var(--global-theme-color); text-decoration: none; font-weight: 500;">Shikai Qiu</span>, Zixi Chen, Hoang Phan, Qi Lei, and Andrew Gordon Wilson</div> <div class="periodical"> <em>arXiv preprint arXiv:2512.05620</em> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2512.05620" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a> </div> <div class="abstract hidden"> <p>Several recently introduced deep learning optimizers utilizing matrix-level preconditioning have shown promising speedups relative to the current dominant optimizer AdamW, particularly in relatively small-scale experiments. However, efforts to validate and replicate their successes have reported mixed results. To better understand the effectiveness of these optimizers at scale, we investigate how to scale preconditioned optimizers via hyperparameter transfer. We study how the optimal learning rate and weight decay should scale with model width and depth for a wide range of optimizers, including Shampoo, SOAP, and Muon, accounting for the impact of commonly used techniques such as blocking and grafting. We find that scaling the learning rate according to μP improves transfer, but can still suffer from significant finite-width deviations. Blocking and spectral normalization help address these deviations, and scaling weight decay inversely with width performs nearly optimally across optimizers. When these scaling rules are applied correctly, Muon and Shampoo deliver approximately 1.4x and 1.3x speedups over AdamW for Llama models ranging from 190M to 1.4B parameters.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="qiu2024collapse" class="col-sm-12"> <div class="title" style="color: var(--global-theme-color);">Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks</div> <div class="author"> <span style="color: var(--global-theme-color); text-decoration: none; font-weight: 500;">Shikai Qiu</span>, Lechao Xiao, Andrew Gordon Wilson, Jeffrey Pennington, and Atish Agarwala</div> <div class="periodical"> <em>International Conference on Machine Learning (ICML), Oral Presentation</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2507.02119" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a> </div> <div class="abstract hidden"> <p>What scaling limits govern neural network training dynamics when model size and training time grow in tandem? We show that despite the complex interactions between architecture, training algorithms, and data, compute-optimally trained models exhibit a remarkably precise universality. Specifically, loss curves from models of varying sizes collapse onto a single universal curve when training compute and loss are normalized to unity at the end of training. With learning rate decay, the collapse becomes so tight that differences in the normalized curves across models fall below the noise floor of individual loss curves across random seeds, a phenomenon we term supercollapse. We observe supercollapse across learning rate schedules, datasets, and architectures, including transformers trained on next-token prediction, and find it breaks down when hyperparameters are scaled suboptimally, providing a precise and practical indicator of good scaling. We explain these phenomena by connecting collapse to the power-law structure in typical neural scaling laws, and analyzing a simple yet surprisingly effective model of SGD noise dynamics that accurately predicts loss curves across various learning rate schedules and quantitatively explains the origin of supercollapse.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="qiu2024compute" class="col-sm-12"> <div class="title" style="color: var(--global-theme-color);">Compute Better Spent: Replacing Dense Layers with Structured Matrices</div> <div class="author"> <span style="color: var(--global-theme-color); text-decoration: none; font-weight: 500;">Shikai Qiu*</span>, Andres Potapczynski*, Marc Finzi, Micah Goldblum, and Andrew Gordon Wilson</div> <div class="periodical"> <em>International Conference on Machine Learning (ICML)</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2406.06248" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a> </div> <div class="abstract hidden"> <p>Dense linear layers are the dominant computational bottleneck in foundation models. Identifying more efficient alternatives to dense matrices has enormous potential for building more compute-efficient models, as exemplified by the success of convolutional networks in the image domain. In this work, we systematically explore structured matrices as replacements for dense matrices. We show that different structures often require drastically different initialization scales and learning rates, which are crucial to performance, especially as models scale. Using insights from the Maximal Update Parameterization, we determine the optimal scaling for initialization and learning rates of these unconventional layers. Finally, we measure the scaling laws of different structures to compare how quickly their performance improves with compute. We propose a novel matrix family containing Monarch matrices, the Block Tensor-Train (BTT), which we show performs better than dense matrices for the same compute on multiple tasks. On CIFAR-10/100 with augmentation, BTT achieves exponentially lower training loss than dense when training MLPs and ViTs. BTT matches dense ViT-S/32 performance on ImageNet-1k with 3.8 times less compute and is more efficient than dense for training small GPT-2 language models.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="gruver2023llmtime" class="col-sm-12"> <div class="title" style="color: var(--global-theme-color);">Large Language Models Are Zero Shot Time Series Forecasters</div> <div class="author"> Nate Gruver*, Marc Finzi*, <span style="color: var(--global-theme-color); text-decoration: none; font-weight: 500;">Shikai Qiu*</span>, and Andrew Gordon Wilson</div> <div class="periodical"> <em>Advances in Neural Information Processing Systems</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2310.07820" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a> </div> <div class="abstract hidden"> <p>By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="qiu2023simple" class="col-sm-12"> <div class="title" style="color: var(--global-theme-color);">Simple and Fast Group Robustness by Automatic Feature Reweighting</div> <div class="author"> <span style="color: var(--global-theme-color); text-decoration: none; font-weight: 500;">Shikai Qiu*</span>, Andres Potapczynski*, Pavel Izmailov, and Andrew Gordon Wilson</div> <div class="periodical"> <em>International Conference on Machine Learning (ICML)</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2306.11074" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a> </div> <div class="abstract hidden"> <p>A major challenge to out-of-distribution generalization is reliance on spurious features – patterns that are predictive of the class label in the training data distribution, but not causally related to the target. Standard methods for reducing the reliance on spurious features typically assume that we know what the spurious feature is, which is rarely true in the real world. Methods that attempt to alleviate this limitation are complex, hard to tune, and lead to a significant computational overhead compared to standard training. In this paper, we propose Automatic Feature Reweighting (AFR), an extremely simple and fast method for updating the model to reduce the reliance on spurious features. AFR retrains the last layer of a standard ERM-trained base model with a weighted loss that emphasizes the examples where the ERM model predicts poorly, automatically upweighting the minority group without group labels. With this simple procedure, we improve upon the best reported results among competing methods trained without spurious attributes on several vision and natural language classification benchmarks, using only a fraction of their compute.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="qiu2023parton" class="col-sm-12"> <div class="title" style="color: var(--global-theme-color);">Parton Labeling without Matching: Unveiling Emergent Labelling Capabilities in Regression Models</div> <div class="author"> <span style="color: var(--global-theme-color); text-decoration: none; font-weight: 500;">Shikai Qiu</span>, Shuo Han, Xiangyang Ju, Benjamin Nachman, and Haichen Wang</div> <div class="periodical"> <em>The European Physical Journal C</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2304.09208" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a> </div> <div class="abstract hidden"> <p>Parton labeling methods are widely used when reconstructing collider events with top quarks or other massive particles. State-of-the-art techniques are based on machine learning and require training data with events that have been matched using simulations with truth information. In nature, there is no unique matching between partons and final state objects due to the properties of the strong force and due to acceptance effects. We propose a new approach to parton labeling that circumvents these challenges by recycling regression models. The final state objects that are most relevant for a regression model to predict the properties of a particular top quark are assigned to said parent particle without having any parton-matched training data. This approach is demonstrated using simulated events with top quarks and outperforms the widely-used \(χ^2\&gt;\)method.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="qiu2023holistic" class="col-sm-12"> <div class="title" style="color: var(--global-theme-color);">Holistic approach to predicting top quark kinematic properties with the covariant particle transformer</div> <div class="author"> <span style="color: var(--global-theme-color); text-decoration: none; font-weight: 500;">Shikai Qiu</span>, Shuo Han, Xiangyang Ju, Benjamin Nachman, and Haichen Wang</div> <div class="periodical"> <em>Physical Review D</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://journals.aps.org/prd/pdf/10.1103/PhysRevD.107.114029" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a> </div> <div class="abstract hidden"> <p>Precise reconstruction of top quark properties is a challenging task at the Large Hadron Collider due to combinatorial backgrounds and missing information. We introduce a physics-informed neural network architecture called the Covariant Particle Transformer (CPT) for directly predicting the top quark kinematic properties from reconstructed final state objects. This approach is permutation invariant and partially Lorentz covariant and can account for a variable number of input objects. In contrast to previous machine learning-based reconstruction methods, CPT is able to predict top quark four-momenta regardless of the jet multiplicity in the event. Using simulations, we show that the CPT performs favorably compared with other machine learning top quark reconstruction approaches.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="atlas2023model" class="col-sm-12"> <div class="title" style="color: var(--global-theme-color);">Model-independent search for the presence of new physics in events including \(H→γγ\&gt; \)with \(\sqrt s\) = 13 TeV pp data recorded by the ATLAS detector at the LHC</div> <div class="author"> ATLAS Collaboration</div> <div class="periodical"> <em>Journal of High Energy Physics</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2301.10486" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Link</a> </div> <div class="abstract hidden"> <p>A model-independent search for new physics leading to final states containing \(H→γγ\&gt;\)decays is performed with 139 fb\(^{−1}\&gt;\)of \(\sqrt s\) = 13 TeV pp collision data recorded by the ATLAS detector at the Large Hadron Collider at CERN. This search examines 22 final states categorized by the objects that are produced in association with the Higgs boson. These objects include isolated electrons or muons, hadronically decaying \(τ\)-leptons, additional photons, missing transverse momentum, and hadronic jets, as well as jets that are tagged as containing a \(b\)-hadron. No significant excesses above Standard Model expectations are observed and limits on the production cross section at 95% confidence level are set. Detector efficiencies are reported for all 22 signal regions, which can be used to convert detector-level cross-section limits reported in this paper to particle-level cross-section constraints.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="https://scholar.google.com/citations?user=pK0OAsQAAAAJ" title="Google Scholar" style="color: #4285F4 !important;" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar" style="color: #4285F4 !important;"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>