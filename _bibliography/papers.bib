@article{finzi2026epiplexity,
  selected={true},
  title={{From Entropy to Epiplexity: Rethinking Information for Computationally Bounded Intelligence}},
  author={Marc Finzi* and Shikai Qiu* and Yiding Jiang* and Pavel Izmailov and J. Zico Kolter and Andrew Gordon Wilson},
  journal={arXiv preprint arXiv:2601.03220},
  annotation={* Equal contribution},
  year={2026},
  abstract={Can we learn more from data than existed in the generating process itself? Can new and useful information be constructed from merely applying deterministic transformations to existing data? On these questions, Shannon information and Kolmogorov complexity come up nearly empty-handed, in part because they assume observers with unlimited computational capacity and fail to target the useful information content. We introduce epiplexity, a formalization of information capturing what computationally bounded observers can learn from data. Epiplexity captures the structural content in data while excluding time-bounded entropy, the random unpredictable content exemplified by pseudorandom number generators and chaotic dynamical systems. We present practical procedures to estimate epiplexity which capture differences across data sources, track with downstream performance, and highlight dataset interventions that improve out-of-distribution generalization.},
  html={https://arxiv.org/abs/2601.03220},
}

@article{qiu2025hyperparameter,
  selected={true},
  title={{Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales}},
  author={Shikai Qiu and Zixi Chen and Hoang Phan and Qi Lei and Andrew Gordon Wilson},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2025},
  abstract={Several recently introduced deep learning optimizers utilizing matrix-level preconditioning have shown promising speedups relative to the current dominant optimizer AdamW, particularly in relatively small-scale experiments. However, efforts to validate and replicate their successes have reported mixed results. To better understand the effectiveness of these optimizers at scale, we investigate how to scale preconditioned optimizers via hyperparameter transfer. We study how the optimal learning rate and weight decay should scale with model width and depth for a wide range of optimizers, including Shampoo, SOAP, and Muon, accounting for the impact of commonly used techniques such as blocking and grafting. We find that scaling the learning rate according to μP improves transfer, but can still suffer from significant finite-width deviations. Blocking and spectral normalization help address these deviations, and scaling weight decay inversely with width performs nearly optimally across optimizers. When these scaling rules are applied correctly, Muon and Shampoo deliver approximately 1.4x and 1.3x speedups over AdamW for Llama models ranging from 190M to 1.4B parameters.},
  html={https://arxiv.org/abs/2512.05620},
}

@article{qiu2024collapse,
  selected={true},
  title={{Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks}},
  author={Shikai Qiu and Lechao Xiao and Andrew Gordon Wilson and Jeffrey Pennington and Atish Agarwala},
  journal={International Conference on Machine Learning (ICML)},
  oral={true},
  year={2024},
  abstract={What scaling limits govern neural network training dynamics when model size and training time grow in tandem? We show that despite the complex interactions between architecture, training algorithms, and data, compute-optimally trained models exhibit a remarkably precise universality. Specifically, loss curves from models of varying sizes collapse onto a single universal curve when training compute and loss are normalized to unity at the end of training. With learning rate decay, the collapse becomes so tight that differences in the normalized curves across models fall below the noise floor of individual loss curves across random seeds, a phenomenon we term supercollapse. We observe supercollapse across learning rate schedules, datasets, and architectures, including transformers trained on next-token prediction, and find it breaks down when hyperparameters are scaled suboptimally, providing a precise and practical indicator of good scaling. We explain these phenomena by connecting collapse to the power-law structure in typical neural scaling laws, and analyzing a simple yet surprisingly effective model of SGD noise dynamics that accurately predicts loss curves across various learning rate schedules and quantitatively explains the origin of supercollapse.},
  html={https://arxiv.org/abs/2507.02119},
}

@article{qiu2024compute,
  selected={true},
    title={{Compute Better Spent: Replacing Dense Layers with Structured Matrices}},
    author={Shikai Qiu* and Andres Potapczynski* and Marc Finzi and Micah Goldblum and Andrew Gordon Wilson},
    journal={International Conference on Machine Learning (ICML)},
    annotation={* Equal contribution},
    year={2024},
    abstract={Dense linear layers are the dominant computational bottleneck in foundation models. Identifying more efficient alternatives to dense matrices has enormous potential for building more compute-efficient models, as exemplified by the success of convolutional networks in the image domain. In this work, we systematically explore structured matrices as replacements for dense matrices. We show that different structures often require drastically different initialization scales and learning rates, which are crucial to performance, especially as models scale. Using insights from the Maximal Update Parameterization, we determine the optimal scaling for initialization and learning rates of these unconventional layers. Finally, we measure the scaling laws of different structures to compare how quickly their performance improves with compute. We propose a novel matrix family containing Monarch matrices, the Block Tensor-Train (BTT), which we show performs better than dense matrices for the same compute on multiple tasks. On CIFAR-10/100 with augmentation, BTT achieves exponentially lower training loss than dense when training MLPs and ViTs. BTT matches dense ViT-S/32 performance on ImageNet-1k with 3.8 times less compute and is more efficient than dense for training small GPT-2 language models.},
      html={https://arxiv.org/abs/2406.06248},
}

@article{qiu2023should,
  title={Should We Learn Most Likely Functions or Parameters?},
  author={Shikai Qiu* and Tim G. J. Rudner* and Sanyam Kapoor* and Andrew Gordon Wilson},
  journal={Advances in Neural Information Processing Systems},
  annotation={* Equal contribution},
  selected={false},
  year={2023},
  abstract={Standard regularized training procedures correspond to maximizing a posterior distribution over parameters, known as maximum a posteriori (MAP) estimation. However, model parameters are of interest only insomuch as they combine with the functional form of a model to provide a function that can make good predictions. Moreover, the most likely parameters under the parameter posterior do not generally correspond to the most likely function induced by the parameter posterior. In fact, we can re-parametrize a model such that any setting of parameters can maximize the parameter posterior. As an alternative, we investigate the benefits and drawbacks of directly estimating the most likely function implied by the model and the data. We show that this procedure leads to pathological solutions when using neural networks and prove conditions under which the procedure is well-behaved, as well as a scalable approximation. Under these conditions, we find that function-space MAP estimation can lead to flatter minima, better generalization, and improved robustness to overfitting.},
  html={https://arxiv.org/abs/2311.15990},
}

@article{gruver2023llmtime,
    title={Large Language Models Are Zero Shot Time Series Forecasters},
    author={Nate Gruver* and Marc Finzi* and Shikai Qiu* and Andrew Gordon Wilson},
    journal={Advances in Neural Information Processing Systems},
    annotation={* Equal contribution},
    year={2023},
    selected={true},
    abstract={By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.},
      html={https://arxiv.org/abs/2310.07820},
}

@article{qiu2023simple,
  title={Simple and Fast Group Robustness by Automatic Feature Reweighting},
  author={Shikai Qiu* and Andres Potapczynski* and Pavel Izmailov and Andrew Gordon Wilson},
  journal={International Conference on Machine Learning (ICML)},
  annotation={* Equal contribution},
  year={2023},
  selected={true},
  abstract={A major challenge to out-of-distribution generalization is reliance on spurious features -- patterns that are predictive of the class label in the training data distribution, but not causally related to the target. Standard methods for reducing the reliance on spurious features typically assume that we know what the spurious feature is, which is rarely true in the real world. Methods that attempt to alleviate this limitation are complex, hard to tune, and lead to a significant computational overhead compared to standard training. In this paper, we propose Automatic Feature Reweighting (AFR), an extremely simple and fast method for updating the model to reduce the reliance on spurious features. AFR retrains the last layer of a standard ERM-trained base model with a weighted loss that emphasizes the examples where the ERM model predicts poorly, automatically upweighting the minority group without group labels. With this simple procedure, we improve upon the best reported results among competing methods trained without spurious attributes on several vision and natural language classification benchmarks, using only a fraction of their compute.},
  html={https://arxiv.org/abs/2306.11074},
}

@article{qiu2023parton,
  title={Parton Labeling without Matching: Unveiling Emergent Labelling Capabilities in Regression Models},
  author={Qiu, Shikai and Han, Shuo and Ju, Xiangyang and Nachman, Benjamin and Wang, Haichen},
  journal={The European Physical Journal C},
  year={2023},
  abstract={Parton labeling methods are widely used when reconstructing collider events with top quarks or other massive particles. State-of-the-art techniques are based on machine learning and require training data with events that have been matched using simulations with truth information. In nature, there is no unique matching between partons and final state objects due to the properties of the strong force and due to acceptance effects. We propose a new approach to parton labeling that circumvents these challenges by recycling regression models. The final state objects that are most relevant for a regression model to predict the properties of a particular top quark are assigned to said parent particle without having any parton-matched training data. This approach is demonstrated using simulated events with top quarks and outperforms the widely-used \(\chi^2\>\) method.},
  html={https://arxiv.org/abs/2304.09208},
  selected={true},
}

@article{qiu2023holistic,
  title={Holistic approach to predicting top quark kinematic properties with the covariant particle transformer},
  author={Qiu, Shikai and Han, Shuo and Ju, Xiangyang and Nachman, Benjamin and Wang, Haichen},
  journal={Physical Review D},
  volume={107},
  number={11},
  pages={114029},
  year={2023},
  publisher={APS},
  abstract={Precise reconstruction of top quark properties is a challenging task at the Large Hadron Collider due to combinatorial backgrounds and missing information. We introduce a physics-informed neural network architecture called the Covariant Particle Transformer (CPT) for directly predicting the top quark kinematic properties from reconstructed final state objects. This approach is permutation invariant and partially Lorentz covariant and can account for a variable number of input objects. In contrast to previous machine learning-based reconstruction methods, CPT is able to predict top quark four-momenta regardless of the jet multiplicity in the event. Using simulations, we show that the CPT performs favorably compared with other machine learning top quark reconstruction approaches.},
  html={https://journals.aps.org/prd/pdf/10.1103/PhysRevD.107.114029},
  selected={true},
}

@article{atlas2023model,
  title={Model-independent search for the presence of new physics in events including \( H\rightarrow\gamma\gamma\> \) with \(\sqrt {s}\) = 13 TeV pp data recorded by the ATLAS detector at the LHC},
  author={ATLAS Collaboration},
  journal={Journal of High Energy Physics},
  year={2023},
  html={https://arxiv.org/abs/2301.10486},
  abstract={A model-independent search for new physics leading to final states containing \(H\rightarrow\gamma\gamma\>\)  decays is performed with 139 fb\(^\{−1\}\>\) of \(\sqrt {s}\) = 13 TeV pp collision data recorded by the ATLAS detector at the Large Hadron Collider at CERN. This search examines 22 final states categorized by the objects that are produced in association with the Higgs boson. These objects include isolated electrons or muons, hadronically decaying \(\tau\)-leptons, additional photons, missing transverse momentum, and hadronic jets, as well as jets that are tagged as containing a \(b\)-hadron. No significant excesses above Standard Model expectations are observed and limits on the production cross section at 95{%} confidence level are set. Detector efficiencies are reported for all 22 signal regions, which can be used to convert detector-level cross-section limits reported in this paper to particle-level cross-section constraints.},
  selected={true}
}
